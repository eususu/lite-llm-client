# lite llm client

This project made for very light llm client.
the main idea is `do not use llm client library`.

# Roadmap

- [x] `2024-07-21` support OpenAI
- [x] `2024-07-25` support Anthropic
- [x] `2024-07-27` add options for inference
- [x] `2024-07-28` support Gemini
- [x] `2024-07-30` support streaming (OpenAI). simple SSE implement.
- [x] `2024-07-31` support streaming (Anthropic). bad SSE implement for just working.
- [ ] support streaming (Gemini)
- [ ] support multimodal (image and text)